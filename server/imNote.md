**思考记录**

---

【2025年3月7日】

- IM

---

围绕着IM开发，主要有三个阶段：第一是如何实现即时通讯的功能；第二是如何让功能更加可靠。第三是如何让功能更加可用。

第一阶段的实现，大概是这样，

（1）设计从受理到读写操作的异步IO处理流程：

```
client: [from, to, message]

while(true)
{
	epoll-wait(tcp-scoket-epoll-fd, ...)
    for(int i = 0; i < tcp-socket-epoll-fd; ++i)
    {
        if ( tcp-socket-epoll-type == READ	)
		{
			from, to, message = tlv_read(...);
			session[to].send(message);
		}
    }
}
```

（2）设计TLV包通信协议避免粘包：

```cpp
send/read: [tag, length, data]
```

（3）实现在线状态的单聊、群聊、好友、离线等功能。

```
client: [from-user-id, to-group-id, message]
while(true)
{
	...
	from, to, message = tlv_read(...);
	group = select_group(to);
	for(session : group.session)
	{
		session.send(message);
		...
	}
}

client: [from, to]
request:
	...
	from, to = tlv_read(...)
	request = [from, to, request-friend-label]
	session[to].send(request)
reply:
	...
	from, to = tlv_read(...)
	user[to].friend.append(to);
	reply = [from, to, request-friend-ok]
	session[to].send(reply);
```

此时的效果是，用户能够顺序的通信，以及正常使用各服务，但过慢，并且重试可能性较高（容易丢失）。

分析一下第一阶段的可靠性，为此涉及到可靠性的定界，即时通讯的可靠性预期效果有三种，（一）最低在发送时，网络环境是正常的，就能够被接收，这意味着发送方只需要确认当时的网络环境和发送动作的正常，无需通过反馈结果来验证。（二）网络环境在发送时刻是异常的，但后续能够回馈修复。（三）若网络环境在发送时刻是异常的，之后不能回馈给用户进行重试（用户无法通过本地验证消息是否到达）。

以此三条为定界，第一阶段的IM处于第三种预期效果。具体可能产生消息丢失的原因有三种：

- 网络传输层导致：网络抖动、连接中断、数据包损坏

  例如，由于连接中断导致在网卡传递字节流的过程中被阻断，一个TLV报文未被完整的发出，导致服务端无法对该报文处理。——这意味着需要一个重传与确认的方案

  - 【2025年3月18日】（详细说明）TCP保证可靠传输的程度，建立在网络环境不会长时间异常。TCP的重传机制在Linux内核中会持续分钟级别的超时重发，这意味若存在导致网络环境长期异常的因素，则会断开连接。故而当客户端或服务端发送后，因其网络环境长期影响而断开，导致之后无法得到处理异常的反馈，因此不可靠。

    > **一、导致长期网络异常的因素及概率**
    >
    > 以下因素可能导致 TCP 连接因长期异常最终断开：
    >
    > 1. 物理层故障（概率：低，但影响严重）
    >
    > - **光缆/光纤中断**：施工挖断光缆、自然灾害（地震、洪水）导致物理链路中断。
    >   - **持续时间**：小时级甚至天级（需人工修复）。
    >   - **示例**：2021年 Facebook 因骨干网路由错误导致全球服务中断6小时。
    >
    > **2. 网络设备故障（概率：中）**
    >
    > - **路由器/交换机宕机**：企业级网络设备因硬件故障或软件BUG崩溃。
    >   - **持续时间**：分钟级至小时级（依赖运维响应速度）。
    >   - **示例**：某机房核心交换机故障，导致内部服务中断30分钟。
    >
    > **3. 运营商网络问题（概率：低，但区域性高）**
    >
    > - **骨干网拥塞或故障**：运营商网络核心节点故障或路由错误。
    >   - **持续时间**：分钟级至小时级。
    >   - **示例**：某运营商 DNS 服务器故障，导致部分省份用户无法访问外网。
    >
    > **4. 防火墙/NAT 超时（概率：高，尤其移动网络）**
    >
    > - **NAT 表项超时**：运营商为节省资源，会清理长时间空闲的 NAT 映射表项。
    >   - **超时时间**：移动网络通常 **5-30分钟**，WiFi 网络可能更长。
    >   - **示例**：用户切换 WiFi 到4G，旧连接的 NAT 映射失效，导致 TCP 连接中断。
    >
    > **5. 严格的企业防火墙策略（概率：中）**
    >
    > - **会话超时**：企业防火墙可能设置短时会话超时（如10分钟），关闭长时间无活动的连接。
    >   - **示例**：用户在公司网络发送消息后，长时间不操作，防火墙断开连接。
    >
    > **6. 移动网络信号盲区（概率：中，依赖场景）**
    >
    > - **信号完全丢失**：进入电梯、地下室、偏远地区等无网络覆盖区域。
    >   - **持续时间**：分钟级至小时级（直到用户离开盲区）。
    >   - **示例**：地铁隧道中无信号，导致 TCP 连接超时断开。
    >
    > ------
    >
    > **二、实际场景中的概率分布**
    >
    > | **场景**                | **发生概率** | **平均恢复时间** | **是否需要应用层处理** |
    > | ----------------------- | ------------ | ---------------- | ---------------------- |
    > | 短暂网络抖动（WiFi/4G） | 高           | 秒级             | 否（TCP 自动恢复）     |
    > | NAT 超时                | 高           | 分钟级           | 是（需重连+重传）      |
    > | 移动信号盲区            | 中           | 分钟级至小时级   | 是（离线消息队列）     |
    > | 骨干网故障              | 低           | 小时级           | 是（冗余链路切换）     |
    > | 物理链路中断            | 极低         | 天级             | 是（灾难恢复机制）     |
    >
    > ------
    >
    > **三、为何应用层仍需处理低概率事件？**
    >
    > 1. **长尾效应**：
    >    即使低概率事件（如光缆中断）发生概率为 0.1%，对于日均 1亿用户的 IM 系统，每天仍有 10万用户受影响。
    > 2. **用户体验**：
    >    用户期望消息100%可靠，尤其是金融、医疗等关键场景。
    > 3. **成本权衡**：
    >    应用层重传机制实现成本低，但能显著提升可靠性。

- 服务端处理层导致：服务端崩溃、线程池阻塞、消息队列溢出

  例如，当服务端崩溃后，队列中的报文无法被处理，从而导致丢失。——这意味着需要一个崩溃处理方案。

- 持久化存储层导致：数据库写入失败、磁盘故障、事务未提交

  例如，发送方发送动作产生后下线，服务端接收到报文进行处理时数据库写入过程失败，用户上线后无法通过反馈进行重传（无法达到预期二）

【2025年3月7日】

因此，可对第一阶段IM得出这样的结论，即第一阶段即时通讯能保证消息有序以及不重复，但不能保证消息可靠，服务可靠。

第二阶段IM解决的问题就在于能够保持消息可靠、服务可靠。对此的解决方案为：

（一）建立SEQ与ACK确认和重传机制——解决网络传输层及持久化存储层问题。基于第一阶段，单服务器对请求队列异步服务处理，这意味着SEQ序列号只需要保证递增即可，通过对请求队列中的消息添加递增序列号，在请求和响应动作中添加定时器以在有限时间等待确认回复，超时则重新发送：

```cpp
request:
	client.tlv_send(messageACK);
	SEQ += 1;
	Timer timer(2s, n);
	AckWaitMap.insert(SEQ, timer)
	timer.start([=](type status){
        if(status == Timer::Cancel){
			[ok!]
        }
        else if(status == Timer::timeout){
            if(timer.timeoutCount > n){
                [close tcp]
            }
            else
            {
                client.tlv_send(messageACK);
                timer.timeoutCount += 1;
                timer.restart();
        	}
        }
    }

response:
	client.tlv_read(req);
	switch(req.type){
    case Request::ACK:
        {
            seq = req.getSEQ();
        	timer = AckWaitMap[seq];
        	timer.cancel();
        }
    }
```

（详细说明【2025年3月8日】

该方案的实现范围在于如消息发送这类不需要反馈多余的信息以为用户处理，但又需要高度的可靠性。其流程的思想在于，一旦服务端发送ACK后没有收到重发的消息时，则对方已接收，这意味着重发的一方只在客户端，在第一次发送失败后的关系是：客户端若重发则是服务端未确认，服务端若未确认则是客户端未重发。

此时仍会出现一个问题：消息重复。消息重复是SEQ与ACK重传机制带来的，在第一阶段时不存在消息重复。原因在于，由于服务端ACK回复后，此时无法确定ACK是否到达客户端，这意味着若此时网络环境异常，客户端等待接收超时后将再发送一次消息，而服务端误以为是新消息，随之处理，故而重复。所以要进而建立服务端校验机制，流程是在可能重复发送这段时间内，消息序列号进入缓存，以在面对重复消息时能够在缓存中查看是否处理，从而拒绝处理消息，并重传以修复ACK并未到达客户端的问题：

```cpp
	message = server.tlv_read(req);
	AckType ack = redisDB.get("over", message.SEQ);
	if(!ack.empty()){
        // 意味着ACK未成功到达客户端
        server.send(ACK);
        int seqExpire = 5s;
        // 延长该序列号的过期时间
        redisDB.hashMap(seqExpire, "over", SEQ, ACK);
    }
	else{
        // 不同点在于是否处理
        handle(message);
        server.send(ACK);
        int seqExpire = 5s;
        // 新增给定时长的序列号
        redisDB.hashMap(seqExpire, "over", SEQ, ACK);
    }
	...
```

）

（二）建立持久化先行的策略——解决服务端处理层的问题（1/2）。服务端在处理请求时崩溃会导致其未上传到数据库的数据（主要是请求队列中的请求）丢失，故而需要在崩溃处理时存储数据。由于无法确认用户在请求服务到服务器崩溃到服务器重连这段时间内是否会客观下线（若有心跳机制，也存在延迟需要时间验证其状态，故而有服务器端视角是未下线，而用户视角则是已下线的情况），这意味对请求数据的存储时机在读取后就需存储、再处理，以保证在用户下线时也能够被新服务器处理，而仅依赖于客户端重新请求则不能。（详细说明【2025年3月8日】：

```cpp
while(true){
    [epoll]
    if(tcp-socket-epoll-type == READ)
    {
		message = tlv_read(...);
        kafkaMemoryMethod(message);
        ...
    }
}

void kafkaMemoryMethod(type message){
    // 持久化手段一
    kafka.put("message-topic", message);
    // 持久化手段二
    /*
    if(sleepPutTimer.status == Timer::Run)
    {
        sleepPutTimer = Timer(5s);
        sleepPutTimer.start([=](type status){
            if(status == Timer::timeout){
                for(auto sleepMessage : SleepMessageQueue){
                    kafka.put("message-topic", sleepMessage);
                }
                kafka.putDB("message-topic", message);
                sleepPutTimer.status = Timer::Stop;
            }
        });
      }
      else if(sleepPutTimer.status == Timer::Wait){
      	SleepMessageQueue.push(message);
      	sleepPutTimer.restart();
	  }
    */
}
```

）

（三）建立备用服务器和用户重连接机制——解决服务端处理层的问题（2/2）。设立一个备用服务器仅为当前服务器崩溃时延续服务，并建立服务器状态注册和监控机制（服务注册和服务发现）来判断是否崩溃。若崩溃则因为客户端请求多次超时，而重连接到备用服务器，并等待一定的时间让服务端处理之前的请求（原因：请求已被上传到队列中，此时重连到备用服务器会受理这些请求，对于在线用户则直接反馈，对于用户此时离线则存入结果到数据库等待上线时反馈）。（详细说明【2025年3月8日】：

```cpp
client:
	....
        else if(status == Timer::timeout){
            if(timer.timeoutCount > n){
                chatSocket.close();
                rsp = gateService.request(Get_Backup_Chat_Server);
                chatSocket.accept(rsp.host, rsp.port);
                
                chatSocket.tlv_read_wait(..., 5s, [=](type status){
                    if(status == Timer::timeout){
                        chatSocket.send(requestMessage);
                    }
                }));
            }
            
backup-server:
	rsp = gateService.tlv_read(...);
	if(rsp.chatServer.status == Server::Bug){
        chat.backopenAcceptor();
        create_thread(chat.HandleRun);
        // chat.pullKafkaRequestQueue():
        requestQueue = kafka.getDB("message-topic");
    	for(auto req : requestQueue){
            chat.requestQueue(req);
        }
    }
```

）

【2025年3月8日】

实现的主要过程在于（1）请求队列上传的时机在于处理前。（2）需要一台服务器以心跳的方式监视正在服务的服务器状态（对于IM阶段二，且只有一台备用服务器的情况下，可直接通过备用服务器监听；对于多个备用服务器的情况下，则需要一台服务器设备以选用备用服务器——通常被称为状态服务器）。（3）备用服务器在得到已正在服务的服务器崩溃信息后拉取请求消息。

【2025年3月18日】（近日被其他事所耽搁了，今续更）

或是用户与服务端之间的心跳，或是故障转移，又或是消息可靠机制，其内在都是ACK重传机制。在建立应用层ACK重传机制前，会话层TCP已有ACK重传机制，TCP能保证顺序、不重复，以及基于重传的可靠性。这意味着，（一）其ACK仅限于确保消息到达对方主机（内核缓冲区），ACK无法确保消息进入应用层后被成功处理。（二）会话级别的TCP超时重传响应不能由应用层控制处理方式，故而无法重连。（三）对于消息的可靠性是基于重传成功的，即当网络环境长期异常以至于重传时间以大于最大超时重传次数时，将会断开连接，此时消息就变得不可靠了（尽管能够调整内核的TCP超时相关因子，但无法精细超时策略）。

【2025年3月20日】

该方案的一些实现阻碍：

- 在这一方案下，基于ACK重传机制的实现主要是两个，（一）心跳机制，从实现上其只需用用户ID标识以找到维护的定时器，以取消重传或清理定时器。（二）消息可靠ACK机制，从用户A到用户B的路径涉及A、服务器、B三个对象，其过程为`[A -> Server, Server -> B， B -> Server, Server -> A`]，其问题就在于服务器怎么回应A，（a）服务器接收到则回复已读；（b）B收到并发送ACK给服务端时则回复A用户已读；若实现后者，并且全程可靠，则需要对三个发送过程都使用ACK重传机制。——其中每个ACK重传机制需要发送端分配一个唯一的消息ID。
- 故障转移方案中，由客户端检查出心跳失常后向网关请求备用服务器主机的连接（即客户端驱动重连），其会产生一个问题，即备用服务器处理持久化队列的请求的处理时机，方法或是（a）等待客户端连接后处理；或是（b）备用服务器一被启用则立即处理；或是（c）为每个客户端维护持久化队列，当客户端重连并发送拉取请求时，定位队列并处理。

【2025年3月22日】

**消息时序——有序性**【注解文章http://www.52im.net/thread-3189-1-1.html】

---

IM消息发送发生乱序，其基本原因在于当在极短的时间内发送多条消息时（无论其是单个发送方还是多个发送方），物理层的电波数据传输是无法按照顺序依次接收的。例如（1）单个发送方短时间发送msg1、msg2两条消息时，由于运营商路由选择的不同，导致路线不一致，以致于后发的msg2消息快于msg1消息；（2）多发送方情况下同时发送消息，目标地在湖南市区，其中A方由于他在湖北，继而从湖北发送，而B方在东北北方，因此在东北发送，这意味着运营商对两位消息的路由路线有远近之别，即使B方早发于A方，只要时差不大，A方更高概率先到达；（3）多接收方情况下，由于各方的地理位置不一，发送方发送的消息路由路线有远近之别，导致消息顺序不一。（4）分布式情况下，即使意从客户端设备上的时间戳作为排序因素，或从服务器的时间戳作为排序因素，由于物理层面上对晶振（设备时间的来源）的干扰，无法保证各服务器或客户端设备的时间一致。

依此造成了IM消息的乱序问题。其解决方案，对于一对一单聊情况让接收端对发送者时间戳排序：

> ​	常见优化方案，在A往B发出的消息中，加上发送方A本地的一个绝对时序（比如本机时间戳），来表示接收方B的展现时序。
>
> 	那么当接收方B收到消息后，即使极端情况下消息可能存在乱序到达，但因为这个乱序的时间差对于普通用户来说体感是很短的，	在UI展现层按照消息中自带的绝对时序排个序后再显示，用户其实是没有太多感知的

因其发送的消息总是由对方决定，时间戳一样由对方决定，因此只需对方的设备时间是递增的即可，这在计算机上的可以保证的。即使在分布式情况下，用户之间不在一个节点，也因只需要保证发出的时序与接收的时序一致，故而服务器之间转发过程的时差则不影响。主要的问题在于接收端怎样选定一个先后消息时差的延迟窗口，以决定间隔多久接收然后排序。比如msg2在200ms到达，msg1到230ms到达，若平均时差为50ms，则选择延迟50ms后读取消息，然后排序，按顺序显示。这个延迟数值则是历史网络延迟的统计值（如P99延迟为200ms），设置一个固定窗口（例如300ms）。——P99是对网络延迟的历史统计值，表示99％的传输操作不会大于该延迟数值。

故而实现“UI展现层按消息自带绝对时序排序显示”的核心思路是：**在客户端引入一个短暂的缓冲延迟窗口（P99），收集可能乱序到达的消息，然后按时间戳排序后统一渲染**。

该方案适用于一对一的情况外，宜适用于一对多的情况，因为多个接收者所接收的只有一个发送者。

对于多对多群聊的解决方案则有（1）建立消息序列号分配的节点以在该节点下取全局唯一的序号的方式解决（下图1）；（2）建立会话级消息传输机制，使群聊的发送入口都通过转发的方式集中到一个固定的服务器节点上，依此分配会话唯一的序号（下图2）。待补充说明。

![零基础IM开发入门(四)：什么是IM系统的消息时序一致性？_x6.jpg](http://www.52im.net/data/attachment/forum/202010/27/152024eje3jngs13ob9vvo.jpg)

![零基础IM开发入门(四)：什么是IM系统的消息时序一致性？_x7.jpg](http://www.52im.net/data/attachment/forum/202010/27/152423t5zwxmme5u804758.jpg)